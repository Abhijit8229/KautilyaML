{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m06KKuMFTZ2g",
        "outputId": "58cb83bf-74d5-4823-dc13-83f3a5496cfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting narrative_builder.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile narrative_builder.py\n",
        "import argparse\n",
        "import json\n",
        "from datetime import datetime\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Load model\n",
        "# -------------------------------------------------------\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Load JSON\n",
        "# -------------------------------------------------------\n",
        "def load_data(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Normalize to list of articles\n",
        "# -------------------------------------------------------\n",
        "\n",
        "from dateutil import parser\n",
        "from datetime import timezone\n",
        "\n",
        "def parse_date(article):\n",
        "    # Extract any possible date field\n",
        "    raw_date = (\n",
        "        article.get(\"date\")\n",
        "        or article.get(\"published_at\")\n",
        "        or article.get(\"created_at\")\n",
        "        or \"\"\n",
        "    )\n",
        "\n",
        "    # If no date or invalid value → fallback\n",
        "    if not raw_date or raw_date in [\"unknown\", \"null\", \"None\", \"-\", \"\"]:\n",
        "        return parser.parse(\"1970-01-01T00:00:00Z\")\n",
        "\n",
        "    try:\n",
        "        dt = parser.parse(raw_date)\n",
        "\n",
        "        # If datetime is naive → assign UTC\n",
        "        if dt.tzinfo is None:\n",
        "            dt = dt.replace(tzinfo=timezone.utc)\n",
        "\n",
        "        return dt.astimezone(timezone.utc)\n",
        "\n",
        "    except Exception:\n",
        "        # If parsing fails → fallback\n",
        "        return parser.parse(\"1970-01-01T00:00:00Z\")\n",
        "\n",
        "\n",
        "def normalize_data(data):\n",
        "    # dataset is {\"items\": [...]}\n",
        "    if isinstance(data, dict) and \"items\" in data:\n",
        "        return data[\"items\"]\n",
        "    # already list\n",
        "    if isinstance(data, list):\n",
        "        return data\n",
        "    return [data]\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Keep only source_rating > 8\n",
        "# -------------------------------------------------------\n",
        "def filter_by_rating(data):\n",
        "\n",
        "    return [d for d in data if d.get(\"source_rating\", 0) > 8]\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Semantic retrieval using title + story\n",
        "# -------------------------------------------------------\n",
        "def get_relevant_articles(data, topic, top_k=30):\n",
        "    topic_emb = model.encode(topic, convert_to_tensor=True)\n",
        "\n",
        "    texts = [(d.get(\"title\", \"\") + \" \" + d.get(\"story\", \"\")) for d in data]\n",
        "    embeddings = model.encode(texts, convert_to_tensor=True)\n",
        "\n",
        "    scores = util.cos_sim(topic_emb, embeddings)[0]\n",
        "    top_results = scores.topk(k=min(top_k, len(scores)))\n",
        "\n",
        "    return [data[int(i)] for i in top_results.indices]\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Narrative summary\n",
        "# -------------------------------------------------------\n",
        "def build_narrative_summary(articles, topic):\n",
        "    if not articles:\n",
        "        return f\"No relevant articles found for topic '{topic}'.\"\n",
        "\n",
        "    summary_sentences = []\n",
        "    for a in articles[:8]:\n",
        "        title = a.get(\"title\", \"Untitled\")\n",
        "        src = a.get(\"source\", \"Unknown source\")\n",
        "        summary_sentences.append(f\"{title} — reported by {src}.\")\n",
        "\n",
        "    return \" \".join(summary_sentences)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Timeline\n",
        "# -------------------------------------------------------\n",
        "def build_timeline(articles):\n",
        "    sorted_articles = sorted(articles, key=lambda a: a.get(\"parsed_date\"))\n",
        "\n",
        "    timeline = []\n",
        "    for a in sorted_articles:\n",
        "        timeline.append({\n",
        "            \"date\": a.get(\"published_at\"),\n",
        "            \"headline\": a.get(\"title\"),\n",
        "            \"url\": a.get(\"url\"),\n",
        "            \"why_it_matters\": a.get(\"story\", \"\")[:200]\n",
        "        })\n",
        "    return timeline\n",
        "\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Clusters\n",
        "# -------------------------------------------------------\n",
        "def build_clusters(articles, num_clusters=4):\n",
        "    if len(articles) < 2:\n",
        "        return {0: articles}\n",
        "\n",
        "    texts = [a.get(\"title\", \"\") for a in articles]\n",
        "    embeddings = model.encode(texts)\n",
        "\n",
        "    if len(embeddings) < num_clusters:\n",
        "        num_clusters = len(embeddings)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=num_clusters, n_init=\"auto\")\n",
        "    labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    clusters = {i: [] for i in range(num_clusters)}\n",
        "    for idx, label in enumerate(labels):\n",
        "        clusters[label].append(articles[idx])\n",
        "\n",
        "    return clusters\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Narrative graph\n",
        "# -------------------------------------------------------\n",
        "def build_graph(articles):\n",
        "    graph = []\n",
        "    for i, a1 in enumerate(articles):\n",
        "        for j, a2 in enumerate(articles):\n",
        "            if i == j:\n",
        "                continue\n",
        "\n",
        "            title1 = a1.get(\"title\", \"\")\n",
        "            title2 = a2.get(\"title\", \"\")\n",
        "\n",
        "            score = util.cos_sim(\n",
        "                model.encode(title1),\n",
        "                model.encode(title2)\n",
        "            ).item()\n",
        "\n",
        "            if score > 0.60:\n",
        "                relation = \"builds_on\"\n",
        "            elif score > 0.45:\n",
        "                relation = \"adds_context\"\n",
        "            else:\n",
        "                relation = \"contradicts\"\n",
        "\n",
        "            graph.append({\n",
        "                \"from\": i,\n",
        "                \"to\": j,\n",
        "                \"relation\": relation,\n",
        "                \"score\": score\n",
        "            })\n",
        "\n",
        "    return graph\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Main\n",
        "# -------------------------------------------------------\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--topic\", required=True)\n",
        "    parser.add_argument(\"--data\", default=\"/content/14e9e4cc-9174-48da-ad02-abb1330b48fe.json\")\n",
        "    parser.add_argument(\"--output\", default=\"/content/narrative_output.json\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Load JSON\n",
        "    raw = load_data(args.data)\n",
        "\n",
        "    # Normalize list format\n",
        "    raw = normalize_data(raw)\n",
        "\n",
        "    # Parse dates\n",
        "    for article in raw:\n",
        "        article[\"parsed_date\"] = parse_date(article)\n",
        "\n",
        "    # Filter\n",
        "    filtered = filter_by_rating(raw)\n",
        "\n",
        "    # Semantically relevant\n",
        "    relevant = get_relevant_articles(filtered, args.topic)\n",
        "\n",
        "    # Clustering\n",
        "    clusters = build_clusters(relevant)\n",
        "\n",
        "    output = {\n",
        "        \"narrative_summary\": build_narrative_summary(relevant, args.topic),\n",
        "        \"timeline\": build_timeline(relevant),\n",
        "        \"clusters\": {\n",
        "            str(k): [{\"title\": art.get(\"title\"), \"url\": art.get(\"url\")} for art in v]\n",
        "            for k, v in clusters.items()\n",
        "        },\n",
        "        \"graph\": build_graph(relevant)\n",
        "    }\n",
        "\n",
        "    # ✨ Write to JSON file\n",
        "    with open(args.output, \"w\") as f:\n",
        "        json.dump(output, f, indent=2)\n",
        "\n",
        "    print(f\"Final narrative JSON written to: {args.output}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python narrative_builder.py --topic \"Jubilee Hills elections\"  --output /content/result.json\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tBVZixfiPW5",
        "outputId": "bcf9b85f-6a29-49a2-89cc-9069226da4cd"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-17 08:42:39.046095: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763368959.099493   46433 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763368959.115917   46433 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763368959.159817   46433 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763368959.159877   46433 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763368959.159882   46433 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763368959.159885   46433 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "/usr/local/lib/python3.12/dist-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname IST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
            "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
            "Final narrative JSON written to: /content/result.json\n"
          ]
        }
      ]
    }
  ]
}